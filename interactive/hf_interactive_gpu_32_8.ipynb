{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3260669-c7ec-4d06-a655-590c5e7ab152",
   "metadata": {},
   "source": [
    "# Transfer learning with Huggingface using CodeFlare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acfb10-1aa1-445d-947e-396ea5ebed1a",
   "metadata": {},
   "source": [
    "In this notebook you will learn how to leverage the **[huggingface](https://huggingface.co/)** support in ray ecosystem to carry out a text classification task using transfer learning. We will be referencing the example **[here](https://huggingface.co/docs/transformers/tasks/sequence_classification)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b77929-e96c-434e-ada3-8b14795bfbb1",
   "metadata": {},
   "source": [
    "The example carries out a text classification task on **[imdb dataset](https://huggingface.co/datasets/imdb)** and tries to classify the movie reviews as positive or negative. Huggingface library provides an easy way to build a model and the dataset to carry out this classification task. In this case we will be using **distilbert-base-uncased** model which is a **BERT** based model.\n",
    "\n",
    "Huggingface has a **[built in support for ray ecosystem](https://docs.ray.io/en/releases-1.13.0/_modules/ray/ml/train/integrations/huggingface/huggingface_trainer.html)** which allows the huggingface trainer to scale on CodeFlare and can scale the training as we add additional gpus and can run distributed training across multiple GPUs that will help scale out the training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02593d04-40b9-4a07-a32e-40b649444ab5",
   "metadata": {},
   "source": [
    "### Getting all the requirements in place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737a768-6e31-4767-a301-60ae932b4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
    "from codeflare_sdk.cluster.auth import TokenAuthentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create authentication object for oc user permissions and login\n",
    "auth = TokenAuthentication(\n",
    "    token = \"sha256~Wclt3EEMNzwGRp6sFrmNcJQSjWi824Cm2bJsd1gjj7Q\",\n",
    "    server = \"https://c130-e.us-south.containers.cloud.ibm.com:30202\",\n",
    "    skip_tls = True\n",
    ")\n",
    "auth.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27f84c",
   "metadata": {},
   "source": [
    "Here, we want to define our cluster by specifying the resources we require for our batch workload. Below, we define our cluster object (which generates a corresponding AppWrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b9d85-3a3c-4c0c-aaf2-0d866823dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our cluster and submit appwrapper\n",
    "cluster = Cluster(ClusterConfiguration(name='hfgputest', min_worker=1, max_worker=3, min_cpus=8, max_cpus=8, min_memory=16, max_memory=16, gpu=1, instascale=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eef53c",
   "metadata": {},
   "source": [
    "Next, we want to bring our cluster up, so we call the `up()` function below to submit our cluster AppWrapper yaml onto the MCAD queue, and begin the process of obtaining our resource cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae1d861-b743-4c05-903b-5799072b942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.up()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ebdfb",
   "metadata": {},
   "source": [
    "Now, we want to check on the initial status of our resource cluster, then wait until it is finally ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0db5f5-22f1-4806-ae7e-a0ee865625c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2969a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ac246",
   "metadata": {},
   "source": [
    "Let's quickly verify that the specs of the cluster are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a54428-f186-4c27-948e-4eaf9c0e34b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac46c87-70f1-4c70-9648-881151665355",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_cluster_uri = cluster.cluster_uri()\n",
    "print(ray_cluster_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dba6a0-8275-4726-8911-6b6ec467b6a3",
   "metadata": {},
   "source": [
    "**NOTE**: Now we have our resource cluster with the desired GPUs, so we can interact with it to train the HuggingFace model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c458589-5a17-47c6-a8db-625427ae4fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before proceeding make sure the cluster exists and the uri is not empty\n",
    "assert ray_cluster_uri, \"Ray cluster needs to be started and set before proceeding\"\n",
    "\n",
    "import ray\n",
    "from ray.air.config import ScalingConfig\n",
    "\n",
    "# reset the ray context in case there's already one. \n",
    "ray.shutdown()\n",
    "# establish connection to ray cluster\n",
    "\n",
    "#install additionall libraries that will be required for this training\n",
    "runtime_env = {\"pip\": [\"scikit-learn\", \"accelerate\", \"transformers\", \"datasets\", \"evaluate\", \"pyarrow<7.0.0\"]}\n",
    "\n",
    "ray.init(address=f'{ray_cluster_uri}', runtime_env=runtime_env)\n",
    "\n",
    "print(\"Ray cluster is up and running: \", ray.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a38146-1321-4b7b-9152-9ebca4eb9444",
   "metadata": {},
   "source": [
    "**NOTE** : in this case since we are running a task for which we need additional pip packages. we can install those by passing them in the `runtime_env` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1945b-d6c8-49b8-9a4c-b82724cffba9",
   "metadata": {},
   "source": [
    "### Transfer learning code from huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbe888-4f38-4e9a-ae43-67ce89ff9d42",
   "metadata": {},
   "source": [
    "We are using the code based on the example **[here](https://huggingface.co/docs/transformers/tasks/sequence_classification)** . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69994b4-1a13-43fe-b698-2a5374cb941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_fn():\n",
    "    from datasets import load_dataset\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, TrainingArguments\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    import numpy as np\n",
    "    from datasets import load_metric\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.train.huggingface import HuggingFaceTrainer\n",
    "\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Using a fraction of dataset but you can run with the full dataset\n",
    "    # hmm, this does not limit to 1000 when we later use the ray.data.from_huggingface\n",
    "    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "    print(f\"len of small_train_dataset {small_train_dataset} and small_eval_dataset {small_eval_dataset}\")\n",
    "\n",
    "    # Using a fraction of dataset - The limit here works\n",
    "    ray_train_ds = ray.data.from_huggingface(small_train_dataset).random_shuffle(seed=42).limit(1000)\n",
    "    ray_evaluation_ds = ray.data.from_huggingface(small_eval_dataset).random_shuffle(seed=42).limit(1000)\n",
    "    \n",
    "    # Using the full dataset\n",
    "    #ray_train_ds = ray.data.from_huggingface(small_train_dataset)\n",
    "    #ray_evaluation_ds = ray.data.from_huggingface(small_eval_dataset)\n",
    "    print(f\"len of ray_train_ds {ray_train_ds} and ray_evaluation_ds {ray_evaluation_ds}\")\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        metric = load_metric(\"accuracy\")\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    def trainer_init_per_worker(train_dataset, eval_dataset, **config):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)#, torchscript=True)\n",
    "\n",
    "        #training_args = TrainingArguments(\"/tmp/hf_imdb/test\", eval_steps=1, disable_tqdm=True, \n",
    "        #                                  num_train_epochs=2, skip_memory_metrics=True,\n",
    "        #                                  learning_rate=2e-5,\n",
    "        #                                  per_device_train_batch_size=16,\n",
    "        #                                  per_device_eval_batch_size=16,                                \n",
    "        #                                  weight_decay=0.01,)\n",
    "        training_args = TrainingArguments(\"results\", disable_tqdm=True, \n",
    "                                          num_train_epochs=3, skip_memory_metrics=True,\n",
    "                                          learning_rate=2e-5,\n",
    "                                          evaluation_strategy=\"steps\",\n",
    "                                          save_strategy = \"no\",\n",
    "                                          logging_strategy = \"steps\",\n",
    "                                          log_level = 'info',\n",
    "                                          logging_first_step = True,\n",
    "                                          logging_steps = 200,\n",
    "                                          eval_steps = 200,\n",
    "                                          per_device_train_batch_size=16,\n",
    "                                          per_device_eval_batch_size=16,                                \n",
    "                                          weight_decay=0.01,)\n",
    "        \n",
    "        return transformers.Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "    scaling_config = ScalingConfig(num_workers=3, use_gpu=True) #num workers is the number of gpus\n",
    "\n",
    "    # we are using the ray native HuggingFaceTrainer, but you can swap out to use non ray Huggingface Trainer. Both have the same method signature. \n",
    "    # the ray native HFTrainer has built in support for scaling to multiple GPUs\n",
    "    trainer = HuggingFaceTrainer(\n",
    "        trainer_init_per_worker=trainer_init_per_worker,\n",
    "        scaling_config=scaling_config,\n",
    "        datasets={\"train\": ray_train_ds, \"evaluation\": ray_evaluation_ds},\n",
    "    )\n",
    "    result = trainer.fit()\n",
    "    print(f\"metrics: {result.metrics}\")\n",
    "    print(f\"checkpoint: {result.checkpoint}\")\n",
    "    print(f\"log_dir: {result.log_dir}\")\n",
    "    return result.checkpoint\n",
    "    #return result.log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9593fee-2b2b-415f-8902-bceec014385f",
   "metadata": {},
   "source": [
    "**NOTE:** This code will produce a lot of output and will run for **approximately 2 minutes.** As a part of execution it will download the `imdb` dataset, `distilbert-base-uncased` model and then will start transfer learning task for training the model with this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0985e9-5e88-4d36-ab38-c3001c13f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the above cell as a remote ray function\n",
    "result=ray.get(train_fn.remote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec421113-0e49-4043-a3b5-66efa5021cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchCheckpoint\n",
    "checkpoint: TorchCheckpoint = result\n",
    "path = checkpoint.to_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c108cd0-4d29-4b7b-856b-e5d961f8afb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path)\n",
    "!ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31f441-c363-435a-bafb-da59454f33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r {path} ./checkpoint2\n",
    "#log_dir=result.log_dir\n",
    "#print(f\"log_dir: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7196c66-3552-4694-b06b-7956b65eeb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"./checkpoint2\"\n",
    "#path=\"/opt/app-root/src/huggingface-checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e6915-908f-44de-80c7-e876c74b946a",
   "metadata": {},
   "source": [
    "# Check if GPU is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393a42f-b27b-45e8-8151-294b759733fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "import onnxruntime as rt\n",
    "print(rt.get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda215e-f8f0-4f41-9000-946219c1e100",
   "metadata": {},
   "source": [
    "# Inference using the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949abe38-377d-46bd-8193-6c628a40f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "text1 = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
    "text2 = \"This is a catastrophe. Each of the three movies had different actors that made it difficult to follow.\"\n",
    "batch=[text1,text2]\n",
    "inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488fef44-db9b-4e3d-885a-834d79678554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c17cb-5d05-46d7-86fc-36ca6e8e653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(path,num_labels=2, id2label=id2label, label2id=label2id)\n",
    "with torch.no_grad(): logits = model(**inputs).logits # For pytorch you have to unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28508028-40f5-4cb3-a864-7649206599d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits)\n",
    "print(torch.nn.Softmax(dim=1)(logits)) #tf.math.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fa12d2-87b1-4ab7-98f3-7220cd4ca717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.array(logits))\n",
    "predicted_class_id = np.array(logits).argmax(axis=1)\n",
    "print(predicted_class_id)\n",
    "print([model.config.id2label[i] for i in predicted_class_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc08aa58-256a-406a-81db-c84bb966267c",
   "metadata": {},
   "source": [
    "# Test the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdbb4b0-c63a-493f-b1b0-85843d746140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import transformers.convert_graph_to_onnx as onnx_convert\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bf7a0-6e2f-41e8-ad47-6dfd57c73017",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\"sentiment-analysis\",model=model,tokenizer=tokenizer)\n",
    "#pipeline = transformers.pipeline(\"text-classification\",model=model,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f81552a-5f14-4eeb-b5d9-34a32920f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline(\"Both the music and visual were astounding, not to mention the actors performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be077897-6dde-4eaf-8121-57d8b24b72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cae3d-9fd0-4c3f-85fc-10656d103ab3",
   "metadata": {},
   "source": [
    "# Convert the model to onnx with and without quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c924553-0211-45d1-92ad-7de1f463cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_convert.convert_pytorch(pipeline, opset=11, output=Path(\"classifier.onnx\"), use_external_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b67feb-ea88-42c3-be9d-4bb46830200a",
   "metadata": {},
   "source": [
    "Due to current limitations in ONNX Runtime, it is not possible to use quantized models with CUDAExecutionProvider https://huggingface.co/docs/optimum/onnxruntime/usage_guides/gpu#use-cuda-execution-provider-with-quantized-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680fb1d7-2a70-42ab-907d-18c998cde818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "quantize_dynamic(\"classifier.onnx\", \"classifier_int8.onnx\", weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390ff72-bc6c-4ff6-8370-e2956ca6fd10",
   "metadata": {},
   "source": [
    "# Test execution of converted onnx model using onnxruntime and with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffbcfb1-4094-4701-b976-0ee8c1797549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "#providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.log_severity_level = 0\n",
    "session = ort.InferenceSession(\"classifier.onnx\",providers=providers,session_options=session_options)\n",
    "session_int8 = ort.InferenceSession(\"classifier_int8.onnx\",providers=providers,session_options=session_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb659c3-00ae-4014-b214-bf58a60bdbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"np\")\n",
    "out = session.run(input_feed=dict(inputs),output_names=['output_0'])[0]\n",
    "out_int8 = session_int8.run(input_feed=dict(inputs),output_names=['output_0'])[0]\n",
    "print('Without quantization',np.argmax(np.array(out),axis=1))\n",
    "print(out)\n",
    "print(torch.nn.Softmax(dim=1)(torch.from_numpy(out)))\n",
    "print('With quantization',np.argmax(np.array(out_int8),axis=1))\n",
    "print(out_int8)\n",
    "print(torch.nn.Softmax(dim=1)(torch.from_numpy(out_int8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c0996-b614-41fb-8b0a-148b28414182",
   "metadata": {},
   "source": [
    "# Test with the imdb test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2a1b1-f361-4ab3-b801-ac23597e2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\") #emotion\n",
    "def tokenize_function(examples):return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b76c7-86b1-40a2-9487-13704de5ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        #while pick in picks: pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    count=0\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel): df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    df['row_num']=picks\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348b01d-1c96-4d63-8ec5-7f8b0fcf5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(dataset[\"train\"],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a84ca5-bb8d-43aa-9233-2b008edbcdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29f3c4-d137-4f29-bbd7-4237ad82ef1f",
   "metadata": {},
   "source": [
    "## Check the accuracy of full test Dataset using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc3968-bd6a-4e0e-aa6d-681e2a1bf99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "sum=0.0\n",
    "batches=len(tokenized_datasets[\"test\"])//100\n",
    "print(batches)\n",
    "for i in range(batches):\n",
    "    partial_eval_dataset = tokenized_datasets[\"test\"][i*100:(i+1)*100]\n",
    "    input_feed = {\"input_ids\": np.array(partial_eval_dataset['input_ids']),\"attention_mask\": np.array(partial_eval_dataset['attention_mask'])}\n",
    "    out = session.run(input_feed=input_feed,output_names=['output_0'])[0]\n",
    "    predictions = np.argmax(out, axis=-1)\n",
    "    m=metric.compute(predictions=predictions, references=partial_eval_dataset['label'])['accuracy']\n",
    "    #print(i*100,m)\n",
    "    sum+=m\n",
    "out = None\n",
    "print(\"Accuracy\",sum/batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c307dcd-ba7d-4212-8898-45e19a9366bf",
   "metadata": {},
   "source": [
    "## Compare with time required for quantized model using a portion of the test DataSet with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676da750-eaaf-40c7-aa87-d10140c7adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_eval_dataset = tokenized_datasets[\"test\"][200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd28713-99cd-4b1f-bb4b-7c0fa27dc81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "input_feed = {\n",
    "    \"input_ids\": np.array(full_eval_dataset['input_ids']),\n",
    "    \"attention_mask\": np.array(full_eval_dataset['attention_mask'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c3c338-8915-4a03-8616-24454ea86601",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Original Model\n",
    "out = session.run(input_feed=input_feed,output_names=['output_0'])[0]\n",
    "predictions = np.argmax(out, axis=-1)\n",
    "print(metric.compute(predictions=predictions, references=full_eval_dataset['label']))\n",
    "out = None\n",
    "predictions=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f88be-df39-4ba0-af0b-ccec34effe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Quantized Model\n",
    "out_int8 = session_int8.run(input_feed=input_feed,output_names=['output_0'])[0]\n",
    "predictions_int8 = np.argmax(out_int8, axis=-1)\n",
    "print(metric.compute(predictions=predictions_int8, references=full_eval_dataset['label']))\n",
    "out_int8 = None\n",
    "predictions_int8=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d61992-513a-43f3-863e-282c5c11cf70",
   "metadata": {},
   "source": [
    "## Compare with time required for quantized model using a portion of the test DataSet with CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fe0da-a0b0-49c8-baf8-502bd035061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers=['CPUExecutionProvider']\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.log_severity_level = 0\n",
    "session = ort.InferenceSession(\"classifier.onnx\",providers=providers,session_options=session_options)\n",
    "session_int8 = ort.InferenceSession(\"classifier_int8.onnx\",providers=providers,session_options=session_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b9dfe-78e6-4a7c-974d-c4bbcc027400",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Original Model\n",
    "out = session.run(input_feed=input_feed,output_names=['output_0'])[0]\n",
    "predictions = np.argmax(out, axis=-1)\n",
    "print(metric.compute(predictions=predictions, references=full_eval_dataset['label']))\n",
    "out = None\n",
    "predictions=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4cb22a-dd1a-4d8f-ac8b-52a8d8f4442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Quantized Model\n",
    "out_int8 = session_int8.run(input_feed=input_feed,output_names=['output_0'])[0]\n",
    "predictions_int8 = np.argmax(out_int8, axis=-1)\n",
    "print(metric.compute(predictions=predictions_int8, references=full_eval_dataset['label']))\n",
    "out_int8 = None\n",
    "predictions_int8=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb642a-f9f4-44e4-819f-13371eb4ddc4",
   "metadata": {},
   "source": [
    "# Preparing the model for Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772089a5-5933-4ebe-81d3-0354aaad2dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the checkpoint path saved to earlier\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path,num_labels=2, id2label=id2label, label2id=label2id, torchscript=True)\n",
    "text1 = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
    "text2 = \"This is a catastrophe. Each of the three movies had different actors that made it difficult to follow.\"\n",
    "batch=[text1,text2]\n",
    "inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "class Pytorch_to_TorchScript(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pytorch_to_TorchScript,self).__init__()\n",
    "        #print(model)\n",
    "        self.model = model\n",
    "    def forward(self,input_ids,attention_mask=None):\n",
    "        x=self.model(input_ids,attention_mask) #[0] #.get('logits')\n",
    "        #x = {'logits': x}\n",
    "        #return x\n",
    "        return x\n",
    "    \n",
    "ptmodel=Pytorch_to_TorchScript().eval()\n",
    "model_scripted = torch.jit.trace(ptmodel,(inputs['input_ids'],inputs['attention_mask']))#,strict=False) # Export to TorchScript\n",
    "# Test the outputs\n",
    "with torch.no_grad():\n",
    "    output_model = ptmodel(inputs['input_ids'],inputs['attention_mask'])\n",
    "output_traced_model = model_scripted(inputs['input_ids'],inputs['attention_mask'])\n",
    "print('output_from_model = ' + str(output_model))\n",
    "print('output_from_traced_model = ' + str(output_traced_model))\n",
    "\n",
    "model_scripted.save('/opt/app-root/src/hfmodel/1/model.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894c86c-4fec-495c-8870-254bc75e4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model_scripted = ' + str(model_scripted))\n",
    "print('model_scripted = ' + str(model_scripted.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72a16f-dba7-4127-854a-d8264adac75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def func(): return [v for k, v in x.items() if k in [\"1\"]]\n",
    "#torch.jit.script(func)\n",
    "#m = torch.jit.script(Pytorch_to_TorchScript())\n",
    "#m.save('hfmodel_scripted.pt') # Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afaf736-9cd0-434a-8a88-a24b98436b89",
   "metadata": {},
   "source": [
    "Do not run this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12798341-d8a5-42c6-a6ac-2b8741c8c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(path,num_labels=2, id2label=id2label, label2id=label2id, torchscript=True)\n",
    "text1 = \"This wasn't a masterpiece. Not completely faithful to the books, but boring from beginning to end. Not my favorite of the three.\"\n",
    "text2 = \"This is a catastrophe. Each of the three movies had different actors that made it difficult to follow.\"\n",
    "batch=[text1,text2]\n",
    "inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_model = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])\n",
    "#output_traced_model = model_scripted(inputs['input_ids'],inputs['attention_mask'])\n",
    "print('output_model = ' + str(output_model))\n",
    "print('output_traced_model = ' + str(output_traced_model))\n",
    "model_scripted = torch.jit.trace(model,(inputs['input_ids'],inputs['attention_mask']))#,strict=False) # Export to TorchScript\n",
    "model_scripted.save('/opt/app-root/src/hfmodel/1/model.pt') # Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe9ff7-51ea-42ce-9d61-7efc8ba04aab",
   "metadata": {},
   "source": [
    "# Verify that the exported model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c9d59-cb5c-4e9c-b66f-4b5d708294b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the outputs by loading the exported TorchScript model\n",
    "loaded_model = torch.jit.load(\"/opt/app-root/src/hfmodel/1/model.pt\")\n",
    "with torch.no_grad():\n",
    "    output_model = loaded_model(inputs['input_ids'],inputs['attention_mask'])\n",
    "output_traced_model = model_scripted(inputs['input_ids'],inputs['attention_mask'])\n",
    "print('output_model = ' + str(output_model))\n",
    "print('output_traced_model = ' + str(output_traced_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e072b2dc-3ed8-4fa9-bb32-4c9e2bf372aa",
   "metadata": {},
   "source": [
    "# Upload Torchscript model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae8216-399c-44b6-8522-0b62244539c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from boto3 import session\n",
    "\n",
    "key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "session = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "s3_client = boto3.client('s3', aws_access_key_id=key_id, aws_secret_access_key=secret_key,endpoint_url=endpoint_url,verify=False)\n",
    "buckets=s3_client.list_buckets()\n",
    "for bucket in buckets['Buckets']: print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ef8e1-310d-4c91-940b-bb7cedbd578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\"/opt/app-root/src/hfmodel/1/model.pt\", bucket['Name'],\"hfmodel/1/model.pt\")\n",
    "s3_client.upload_file(\"/opt/app-root/src/hfmodel/config.pbtxt\", bucket['Name'],\"hfmodel/config.pbtxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b898b1-e014-4161-8cf5-262d6e9333f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[item.get(\"Key\") for item in s3_client.list_objects_v2(Bucket=bucket['Name']).get(\"Contents\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d23f97-5244-4a2c-b115-aa1710d9f0f1",
   "metadata": {},
   "source": [
    "Now manually deploy the model from Data Science Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992216f-ac9e-43cb-839b-92c25778a561",
   "metadata": {},
   "source": [
    "# Convert to onyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71df32-ab7f-403e-83e2-c44c9fc52fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model, \n",
    "    tuple(inputs.values()),\n",
    "    f=\"torch-model.onnx\",  \n",
    "    input_names=['input_ids', 'attention_mask'], \n",
    "    output_names=['logits'], \n",
    "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'attention_mask': {0: 'batch_size', 1: 'sequence'}, \n",
    "                  'logits': {0: 'batch_size', 1: 'sequence'}}, \n",
    "    do_constant_folding=True, \n",
    "    opset_version=13, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b0017-6f6b-4c4b-93b4-060a83d246b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6efd7-a378-4fbc-8c9a-dd6ef1b88226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace73dbb-d017-41ec-a7f8-bbc58879eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bc496-ef5f-4a27-93af-5056cc749e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(tokenizer)\n",
    "\n",
    "session = onnxruntime.InferenceSession('torch-model.onnx', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "text=\"This is a catastrophe.\"\n",
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "print(inputs)\n",
    "\n",
    "result1 = session.run([i.name for i in session.get_outputs()], dict(inputs))\n",
    "print(result1)\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "predicted_class_id = np.array(result1).argmax().item()\n",
    "print(id2label[predicted_class_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3334d19-c11e-444e-91ca-d1baf1a9eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#predictions = tf.math.softmax(result, axis=-1)\n",
    "print(torch.nn.Softmax(dim=1)(torch.tensor(result1[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282eeb6-eae7-48f9-b357-875d41da45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
    "text2 = \"This is a catastrophe.\"\n",
    "batch=[text1,text2]\n",
    "inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"np\")\n",
    "print(inputs)\n",
    "result2 = session.run([i.name for i in session.get_outputs()], dict(inputs))\n",
    "print(result2)\n",
    "torch.nn.Softmax(dim=1)(torch.tensor(result2[0]))\n",
    "print(np.argmax(torch.nn.Softmax(dim=1)(torch.tensor(result2[0])),axis=1))\n",
    "print([id2label[i.item()] for i in torch.argmax(torch.nn.Softmax(dim=1)(torch.tensor(result2[0])),axis=1)])\n",
    "labels=[id2label[labelid] for labelid in torch.argmax(torch.nn.Softmax(dim=1)(torch.tensor(result2[0])),axis=1).tolist()]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe649550-8536-4624-a9c8-9ef3b19b527c",
   "metadata": {},
   "source": [
    "# Upload the onnx model and quantized onnx model to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e15cc-cde4-49d5-a16a-f42d6feb6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from boto3 import session\n",
    "\n",
    "key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "session = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "s3_client = boto3.client('s3', aws_access_key_id=key_id, aws_secret_access_key=secret_key,endpoint_url=endpoint_url,verify=False)\n",
    "buckets=s3_client.list_buckets()\n",
    "for bucket in buckets['Buckets']: print(bucket['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04e2fe-e37d-4565-9bc8-dcad76e1cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bucket['Name'])\n",
    "#modelfile='torch-model.onnx'\n",
    "#s3_client.upload_file(modelfile, bucket['Name'],'hf_model.onnx')\n",
    "s3_client.upload_file(\"classifier.onnx\", bucket['Name'],'classifier.onnx')\n",
    "s3_client.upload_file(\"classifier_int8.onnx\", bucket['Name'],'classifier_int8.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425943e1-6c53-4bae-b557-70ea314156a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[item.get(\"Key\") for item in s3_client.list_objects_v2(Bucket=bucket['Name']).get(\"Contents\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef500bc-3cdb-4db8-9173-b99ff4035c26",
   "metadata": {},
   "source": [
    "Now manually deploy the model from Data Science Projects\n",
    "\n",
    "---\n",
    "# Submit inferencing request to Deployed model using HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b99b4-48a4-4e51-9714-d5e7ab7e862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "URL='http://modelmesh-serving.huggingface.svc.cluster.local:8008/v2/models/hfmodel/infer' # underscore characters are removed\n",
    "headers = {}\n",
    "payload = {\n",
    "        \"inputs\": [{ \"name\": \"input_ids\", \"shape\": inputs.get('input_ids').shape, \"datatype\": \"INT32\", \"data\": inputs.get('input_ids').tolist()},{ \"name\": \"attention_mask\", \"shape\": inputs.get('attention_mask').shape, \"datatype\": \"INT8\", \"data\": inputs.get('attention_mask').tolist()}]\n",
    "    }\n",
    "print(payload)\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "res = requests.post(URL, json=payload, headers=headers)\n",
    "print(res)\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c4865-15c2-45f1-891e-5ddf6edc5637",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[np.array(res.json().get('outputs')[0].get('data')).reshape(res.json().get('outputs')[0].get('shape'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9932ffb6-981e-439f-a568-b1fc3a26b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Softmax(dim=1)(torch.tensor(result[0]))\n",
    "print(np.argmax(torch.nn.Softmax(dim=1)(torch.tensor(result[0])),axis=1))\n",
    "print('Using item',[id2label[i.item()] for i in torch.argmax(torch.nn.Softmax(dim=1)(torch.tensor(result[0])),axis=1)])\n",
    "labels=[id2label[labelid] for labelid in torch.argmax(torch.nn.Softmax(dim=1)(torch.tensor(result[0])),axis=1).tolist()]\n",
    "print('Using to_list',labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af68919-6708-480e-84ce-277bdd1b74a8",
   "metadata": {},
   "source": [
    "# Submit batches of inferencing requests to Deployed model using HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1a85a-818c-4ae1-bc96-5016d56acd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import requests\n",
    "import json\n",
    "URL='http://modelmesh-serving.huggingface.svc.cluster.local:8008/v2/models/hfmodel/infer' # underscore characters are removed\n",
    "headers = {}\n",
    "import numpy as np\n",
    "sum=0.0\n",
    "batch_size=250\n",
    "batches=len(tokenized_datasets[\"test\"])//batch_size\n",
    "print(batches)\n",
    "for batchnum in range(batches):\n",
    "    partial_eval_dataset = tokenized_datasets[\"test\"][batchnum*batch_size:(batchnum+1)*batch_size]\n",
    "    payload = {\n",
    "            \"inputs\": [{ \"name\": \"input_ids\", \"shape\": (batch_size,512), \"datatype\": \"INT32\", \"data\": partial_eval_dataset['input_ids']},\n",
    "                       { \"name\": \"attention_mask\", \"shape\": (batch_size,512), \"datatype\": \"INT8\", \"data\": partial_eval_dataset['attention_mask']}]\n",
    "        }\n",
    "    #print(payload)\n",
    "    headers = {\"content-type\": \"application/json\"}\n",
    "    res = requests.post(URL, json=payload, headers=headers)\n",
    "    #print(res.json())#.get[\"outputs\"])#[0].get[\"data\"])\n",
    "    predictions = np.argmax(np.array(res.json()[\"outputs\"][0][\"data\"]).reshape(*[int(i) for i in res.json()[\"outputs\"][0][\"shape\"]]), axis=1)\n",
    "    m=metric.compute(predictions=predictions, references=partial_eval_dataset['label'])['accuracy']\n",
    "    print(batchnum,m)\n",
    "    sum+=m\n",
    "print(\"Accuracy\",sum/batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea921aa-60fc-4495-8430-fdc2ad40f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "URL='http://modelmesh-serving.huggingface.svc.cluster.local:8008/v2/models/hfmodel/infer' # underscore characters are removed\n",
    "headers = {}\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n",
    "length_eval = len(eval_dataset)\n",
    "batch_size = 250\n",
    "n_steps = int(length_eval/batch_size)\n",
    "count=0\n",
    "\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for i in range(0,n_steps):\n",
    "        print (i,\"/\",n_steps)\n",
    "        small_eval_dataset = eval_dataset.select(range(i*batch_size,(i+1)*batch_size))\n",
    "        batch_x = small_eval_dataset['text']\n",
    "        batch_y = small_eval_dataset['label']\n",
    "        inputs = tokenizer(batch_x, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        payload = {\n",
    "        \"inputs\": [{ \"name\": \"input_ids\", \n",
    "                    \"shape\": inputs.get('input_ids').shape, \n",
    "                    \"datatype\": \"INT32\", \n",
    "                    \"data\": inputs.get('input_ids').tolist()},\n",
    "                   { \"name\": \"attention_mask\", \n",
    "                    \"shape\": inputs.get('attention_mask').shape, \n",
    "                    \"datatype\": \"INT8\", \n",
    "                    \"data\": inputs.get('attention_mask').tolist()}]}\n",
    "        \n",
    "        headers = {\"content-type\": \"application/json\"}\n",
    "        res = requests.post(URL, json=payload, headers=headers, verify = False)\n",
    "\n",
    "\n",
    "        result =[np.array(res.json().get('outputs')[0].get('data')).reshape(res.json().get('outputs')[0].get('shape'))]\n",
    "        \n",
    "        predicted_class_id = np.argmax(result[0],axis=1)\n",
    "        #print(predicted_class_id==np.array(batch_y))\n",
    "        count_batch = (predicted_class_id == np.array(batch_y)).sum()\n",
    "        count = count + count_batch\n",
    "        print (f'Batch {count_batch}/{batch_size} Total {count}/{batch_size*(i+1)}')\n",
    "        \n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "print (\"Accuracy is \", count/length_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbb860-dc0f-4f4c-b9ee-f66468972565",
   "metadata": {},
   "source": [
    "# Submit batches of inferencing requests to Deployed model using GRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170e88c-0f43-4ed4-abbc-e0a13f5993dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grpcio grpcio-tools==1.46.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fcc311-30d1-4de3-bb99-4ae5545fb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/kserve/kserve/master/docs/predict-api/v2/grpc_predict_v2.proto\n",
    "!wget https://raw.githubusercontent.com/kserve/modelmesh-serving/main/fvt/proto/kfs_inference_v2.proto\n",
    "!python3 -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. ./kfs_inference_v2.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e63c8-63cd-4708-b6ac-c1b48695e7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import grpc\n",
    "import kfs_inference_v2_pb2, kfs_inference_v2_pb2_grpc\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "import struct\n",
    "import base64\n",
    "\n",
    "FLOAT = 'f'\n",
    "grpc_url=\"modelmesh-serving.huggingface.svc.cluster.local:8033\"\n",
    "grpc_channel = grpc.insecure_channel(grpc_url)\n",
    "grpc_stub = kfs_inference_v2_pb2_grpc.GRPCInferenceServiceStub(grpc_channel)\n",
    "\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n",
    "length_eval = len(eval_dataset)\n",
    "batch_size = 440\n",
    "n_steps = int(length_eval/batch_size)\n",
    "\n",
    "count=0\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for i in range(0,n_steps):\n",
    "        print (i,\"/\",n_steps)\n",
    "        small_eval_dataset = eval_dataset.select(range(i*batch_size,min((i+1)*batch_size,len(eval_dataset))))\n",
    "        batch_x = small_eval_dataset['text']\n",
    "        batch_y = small_eval_dataset['label']\n",
    "        inputs = tokenizer(batch_x, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        \n",
    "        payload = { \"model_name\": \"classifier\",\n",
    "        \"inputs\": [{ \"name\": \"input_ids\", \"shape\": inputs.get('input_ids').shape, \"datatype\": \"INT32\", \n",
    "                     \"contents\": {\"int_contents\":[y for x in inputs.get('input_ids').tolist() for y in x]}},\n",
    "                   { \"name\": \"attention_mask\", \"shape\": inputs.get('attention_mask').shape, \"datatype\": \"INT8\", \n",
    "                     \"contents\": {\"int_contents\":[y for x in inputs.get('attention_mask').tolist() for y in x]}}]\n",
    "                  }\n",
    "        request=kfs_inference_v2_pb2.ModelInferRequest(model_name=\"hfmodel\",inputs=payload[\"inputs\"])\n",
    "        response = grpc_stub.ModelInfer(request)\n",
    "\n",
    "        d = MessageToDict(response.outputs[0])\n",
    "        #print(d)\n",
    "        binary_data=bytes([x for x in response.raw_output_contents[0]])\n",
    "        fmt = '<' + FLOAT * (len(binary_data) // struct.calcsize(FLOAT))\n",
    "        numbers = struct.unpack(fmt, binary_data)\n",
    "        #print('numbers',numbers)\n",
    "\n",
    "        predicted_class_id = np.array(numbers).reshape(*[int(i) for i in d.get(\"shape\")]).argmax(axis=1)\n",
    "        #print('predicted_class_id',predicted_class_id,np.array(batch_y))\n",
    "        #print(predicted_class_id,np.array(batch_y),predicted_class_id==np.array(batch_y))\n",
    "        count_batch = (predicted_class_id == np.array(batch_y)).sum()\n",
    "        count = count + count_batch\n",
    "        print (f'Batch {count_batch}/{min((i+1)*batch_size,len(eval_dataset))-i*batch_size} Total {count}/{min(batch_size*(i+1),len(eval_dataset))}')\n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "print (\"Accuracy is \", count/length_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a0ef8-8cec-43c0-9984-c787d1d9c913",
   "metadata": {},
   "source": [
    "# Test single payload using gRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5633d01f-c033-452f-bb42-c376987b0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n",
    "text2 = \"This is a catastrophe.\"\n",
    "batch=[text1,text2]\n",
    "inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"np\")\n",
    "print(inputs)\n",
    "payload = { \"model_name\": \"hfmodel\",\n",
    "        \"inputs\": [{ \"name\": \"input_ids\", \"shape\": inputs.get('input_ids').shape, \"datatype\": \"INT32\", \n",
    "                     \"contents\": {\"int_contents\":[y for x in inputs.get('input_ids').tolist() for y in x]}},\n",
    "                   { \"name\": \"attention_mask\", \"shape\": inputs.get('attention_mask').shape, \"datatype\": \"INT8\", \n",
    "                     \"contents\": {\"int_contents\":[y for x in inputs.get('attention_mask').tolist() for y in x]}}]\n",
    "    }\n",
    "print(json.dumps(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb63c22-5b51-456a-88bb-f151e68a6a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import kfs_inference_v2_pb2, kfs_inference_v2_pb2_grpc\n",
    "grpc_url=\"modelmesh-serving.huggingface.svc.cluster.local:8033\"\n",
    "request=kfs_inference_v2_pb2.ModelInferRequest(model_name=\"hfmodel\",inputs=payload[\"inputs\"])\n",
    "grpc_channel = grpc.insecure_channel(grpc_url)\n",
    "grpc_stub = kfs_inference_v2_pb2_grpc.GRPCInferenceServiceStub(grpc_channel)\n",
    "response = grpc_stub.ModelInfer(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a151141-2a10-4d97-a420-dc28f65432a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(response.outputs),type(response.raw_output_contents))\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "d = MessageToDict(response.outputs[0])\n",
    "print(d)\n",
    "binary_data=bytes([x for x in response.raw_output_contents[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba3e10-4d74-45dd-b7d0-0c27da4304c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import base64\n",
    "FLOAT = 'f'\n",
    "fmt = '<' + FLOAT * (len(binary_data) // struct.calcsize(FLOAT))\n",
    "numbers = struct.unpack(fmt, binary_data)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3cac62-37b6-47ec-9815-cad2e930ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(numbers).reshape(*[int(i) for i in d.get(\"shape\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8cd32",
   "metadata": {},
   "source": [
    "Finally, we bring our resource cluster down and release/terminate the associated resources, bringing everything back to the way it was before our cluster was brought up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a183b-5e8e-4adb-b9a6-a349e13512a0",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As shown in the above example, you can easily run your Huggingface transfer learning tasks easily and natively on CodeFlare. You can scale them from 1 to n GPUs without requiring you to make any significant code changes and leveraging the native Huggingface trainer. \n",
    "\n",
    "Also refer to additional notebooks that showcase other use cases\n",
    "In our next notebook [./02_codeflare_workflows_encoding.ipynb ] shows an sklearn example and how you can leverage workflows to run experiment pipelines and explore multiple pipelines in parallel on CodeFlare cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630e78d9-ed9e-487f-b933-adde53ae5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f8933-5999-4f75-9626-f292d60356df",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.logout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
