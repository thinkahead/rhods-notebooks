{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55bc3ea-4ce3-49bf-bb1f-e209de8ca47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pieces from codeflare-sdk\n",
    "from codeflare_sdk.cluster.cluster import Cluster, ClusterConfiguration\n",
    "from codeflare_sdk.cluster.auth import TokenAuthentication\n",
    "from codeflare_sdk.job.jobs import DDPJobDefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614daa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create authentication object for oc user permissions\n",
    "auth = TokenAuthentication(\n",
    "    token = \"sha256~jnyicQGsSMtmoyxBWyuhH-2_Av4KmeQ63IiaDCsE1mY\",\n",
    "    server = \"https://api.mini2.mydomain.com:6443\",\n",
    "    skip_tls=True\n",
    ")\n",
    "auth.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc27f84c",
   "metadata": {},
   "source": [
    "Here, we want to define our cluster by specifying the resources we require for our batch workload. Below, we define our cluster object (which generates a corresponding AppWrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4bc870-091f-4e11-9642-cba145710159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our cluster and submit appwrapper (reduce specs as desired)\n",
    "cluster = Cluster(ClusterConfiguration(\n",
    "    name='mnisttest',\n",
    "    namespace='batch-mnist',\n",
    "    image=\"quay.io/thinkahead/base:ray2.1.0-py38-gpu-pytorch1.12.0cu117-20230419-1\",\n",
    "    min_worker=2,\n",
    "    max_worker=3,\n",
    "    min_cpus=8,\n",
    "    max_cpus=8,\n",
    "    min_memory=16,\n",
    "    max_memory=16,\n",
    "    gpu=1,\n",
    "    instascale=False # Can be set to false if scaling not needed\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eef53c",
   "metadata": {},
   "source": [
    "Next, we want to bring our cluster up, so we call the `up()` function below to submit our cluster AppWrapper yaml onto the MCAD queue, and begin the process of obtaining our resource cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0884bbc-c224-4ca0-98a0-02dfa09c2200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring up the cluster\n",
    "cluster.up()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657ebdfb",
   "metadata": {},
   "source": [
    "Now, we want to check on the status of our resource cluster, and wait until it is finally ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b4311-2e61-44c9-8225-87c2db11363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.wait_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a55fe4",
   "metadata": {},
   "source": [
    "Let's quickly verify that the specs of the cluster are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd45bc5-03c0-4ae5-9ec5-dd1c30f1a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.details()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2c9b3",
   "metadata": {},
   "source": [
    "Now that our resource cluster is ready, we can directly submit our batch job (model training on three workers with 1 gpu each) to the cluster via torchx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6ccd6-a17e-413a-a0e4-65004fc35463",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "jobdef = DDPJobDefinition(\n",
    "    name=\"cifar10\",\n",
    "    script=\"cifar10.py\",\n",
    "    env={'AWS_ACCESS_KEY_ID':os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "         'AWS_SECRET_ACCESS_KEY':os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "         'AWS_S3_ENDPOINT':os.environ.get('AWS_S3_ENDPOINT'),\n",
    "         'OUTPUT_PATH':'saved/cifar10.onnx'},\n",
    "    #scheduler_args={\"requirements\": \"requirements.txt\"}\n",
    ")\n",
    "job = jobdef.submit(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff065051",
   "metadata": {},
   "source": [
    "Now we can go ahead and look at the status and logs of our batch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0b0da-c22e-4142-b096-407ac8aebe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c1809-de72-4acf-b0f6-e67d345640f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job.logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8cd32",
   "metadata": {},
   "source": [
    "Finally, we bring our resource cluster down and release/terminate the associated resources, bringing everything back to the way it was before our cluster was brought up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f36db0f-31f6-4373-9503-dc3c1c4c3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.logout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eabf4f1-8a8d-41a6-9366-75ed4bf8f898",
   "metadata": {},
   "source": [
    "# Load the CIFAR10 dataset using torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7178bb-7790-4683-88c8-f2a614630148",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5997a1e-b3b4-4daf-ac24-463db5c4a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "trainX=CIFAR10(\"..\", train=True, download=True)\n",
    "testX=CIFAR10(\"..\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ad177-3cd5-4cbb-b27d-5ff9427ebd90",
   "metadata": {},
   "source": [
    "Visualize first 25 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008c8e1-48b3-4eb1-b8da-046be1bf4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "# label names of the images\n",
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# define rows and columns of figure\n",
    "rows, columns = 5, 5\n",
    "fig=plt.figure(figsize=(10, 10))\n",
    "# visualize these first 25 images\n",
    "for i in range(1, columns*rows +1):\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    img,label=testX[i-1]\n",
    "    #plt.imshow(np.array(img).transpose(1,2,0).reshape(32,32,3))\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"{}\".format(label_names[label]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38262f9b-6ef6-423f-82b8-00fb3e3c1c25",
   "metadata": {},
   "source": [
    "Check the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4769fb46-8809-4030-965d-3f85447de688",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = testX[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff109c-9375-40e9-8a73-4ecd027de657",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468090b-11cc-4782-9cf1-45fd38b49c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the PIL image to a PyTorch tensor using ToTensor() and plot the pixel values of this tensor image. \n",
    "# We define our transform function to convert the PIL image to a PyTorch tensor image.\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "img, label = testX[99]\n",
    "# define custom transform function\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    " \n",
    "# transform the pIL image to tensor\n",
    "# image\n",
    "img_tr = transform(img)\n",
    " \n",
    "# Convert tensor image to numpy array\n",
    "img_np = np.array(img_tr)\n",
    " \n",
    "# plot the pixel values\n",
    "plt.hist(img_np.ravel(), bins=50, density=True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")\n",
    "\n",
    "# calculate mean and std\n",
    "mean, std = img_tr.mean([1,2]), img_tr.std([1,2])\n",
    " \n",
    "# print mean and std\n",
    "print(\"mean and std before normalize:\")\n",
    "print(\"Mean of the image:\", mean)\n",
    "print(\"Std of the image:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a028ff-67a8-453c-9159-c5664e2d5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    " \n",
    "# define custom transform\n",
    "# here we are using our calculated\n",
    "# mean & std\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    " \n",
    "# get normalized image\n",
    "img_normalized = transform_norm(img)\n",
    " \n",
    "# convert normalized image to numpy\n",
    "# array\n",
    "img_np = np.array(img_normalized)\n",
    " \n",
    "# plot the pixel values\n",
    "plt.hist(img_np.ravel(), bins=50, density=True)\n",
    "plt.xlabel(\"pixel values\")\n",
    "plt.ylabel(\"relative frequency\")\n",
    "plt.title(\"distribution of pixels\")\n",
    "\n",
    "# Python code to calculate mean and std\n",
    "# of normalized image\n",
    " \n",
    "# get normalized image\n",
    "img_nor = transform_norm(img)\n",
    " \n",
    "# cailculate mean and std\n",
    "mean, std = img_nor.mean([1,2]), img_nor.std([1,2])\n",
    " \n",
    "# print mean and std\n",
    "print(\"Mean and Std of normalized image:\")\n",
    "print(\"Mean of the image:\", mean)\n",
    "print(\"Std of the image:\", std)\n",
    "\n",
    "# Here we find that after normalization the values of mean and std are 0.0 and 1.0 respectively. \n",
    "# This verifies that after normalize the image mean and standard deviation becomes 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c1f83-2600-4ca4-b157-94311639e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code to visualize normalized image\n",
    " \n",
    "# get normalized image\n",
    "img_normalized = transform_norm(img)\n",
    " \n",
    "# convert this image to numpy array\n",
    "img_normalized = np.array(img_normalized)\n",
    " \n",
    "# transpose from shape of (3,,) to shape of (,,3)\n",
    "img_normalized = img_normalized.transpose(1, 2, 0)\n",
    " \n",
    "# display the normalized image\n",
    "plt.imshow(img_normalized)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48146003-536c-44de-bf87-15adf3e970a1",
   "metadata": {},
   "source": [
    "Copy cifar10.onnx from the Ray pod to this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dd032-f906-4a95-94bc-c47ab969d0fc",
   "metadata": {},
   "source": [
    "# Load the onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157aad9-3da1-4c93-ac1f-6d3182356c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='cifar10' # torch.onnx.export with batch\n",
    "model_file_name=model_name+\".onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db22506-bb64-4e68-b819-b3a40efef834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import torch\n",
    "import numpy as np\n",
    "session = onnxruntime.InferenceSession(model_file_name, None, providers=['CPUExecutionProvider'])\n",
    "input_name = session.get_inputs()[0].name\n",
    "print(\"input name\", input_name)\n",
    "input_shape = session.get_inputs()[0].shape\n",
    "print(\"input shape\", input_shape)\n",
    "input_type = session.get_inputs()[0].type\n",
    "print(\"input type\", input_type)\n",
    "print([i.name for i in session.get_outputs()])\n",
    "output_name = session.get_outputs()[0].name\n",
    "print(\"output name\", output_name)\n",
    "output_shape = session.get_outputs()[0].shape\n",
    "print(\"output shape\", output_shape)\n",
    "output_type = session.get_outputs()[0].type\n",
    "print(\"output type\", output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6603287-70cc-4d42-b6fa-ca74a580d5cd",
   "metadata": {},
   "source": [
    "# Inferencing using onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2907efa-6c2c-4fe8-9d96-a4adf45a6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# define custom transform function\n",
    "transform = transforms.Compose([\n",
    "    #transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5,0.5))\n",
    "    transforms.ToTensor(),transforms.Normalize(mean=(0.49139968, 0.48215827 ,0.44653124),std=(0.24703233, 0.24348505, 0.26158768))\n",
    "])\n",
    "\n",
    "count=0\n",
    "total_count=25\n",
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# define rows and columns of figure\n",
    "rows, columns = 5, 5\n",
    "fig=plt.figure(figsize=(10, 10))\n",
    "# visualize these first 25 images\n",
    "for i in range(1, columns*rows +1):\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    img,expected=testX[i-1]\n",
    "    result = session.run([i.name for i in session.get_outputs()], {input_name:np.array(transform(np.array(img))).reshape(1,3,32,32)})\n",
    "    actual=np.argmax(result)\n",
    "    #plt.imshow(np.array(img).transpose(1,2,0).reshape(32,32,3))\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"{} ({})\".format(label_names[expected],label_names[actual]),color=(\"green\" if expected==actual else \"red\"))\n",
    "    if actual!=expected: count+=1\n",
    "    #print(\"Expected\",expected,\"Actual\",actual,actual==expected)\n",
    "print('Accuracy:',(1-count/total_count))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39a92f-a2d8-4ff2-b960-4aacba121251",
   "metadata": {},
   "source": [
    "# Fetch the CIFAR10 dataset from scikit-learn (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0adb2-f22b-411f-84a2-9b4534192aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b9443-c626-41a0-9686-44bdd217be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "X, y = fetch_openml('CIFAR_10', return_X_y=True, parser='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e3f00f-b5a8-47f0-9742-83e43bf03b6b",
   "metadata": {},
   "source": [
    "## Draw few images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d06cf1-f1ce-46d6-8d79-36c308ecb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "# reshape and transpose the images\n",
    "images = np.array(X[0:25]).reshape(25,3,32,32).transpose(0,2,3,1)\n",
    "# take labels of the images \n",
    "labels = y\n",
    "# label names of the images\n",
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# define rows and columns of figure\n",
    "rows, columns = 5, 5\n",
    "fig=plt.figure(figsize=(10, 10))\n",
    "# visualize these first 25 images\n",
    "for i in range(1, columns*rows +1):\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(images[i-1].reshape(32,32,3))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"{}\".format(label_names[int(y.iloc[i-1])]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2560e45f-e718-4f6b-8463-37a7236435dc",
   "metadata": {},
   "source": [
    "# Copy onnx model to S3 bucket if not already copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ceef17-48fc-4fcc-89bf-971b9cafa217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from boto3 import session\n",
    "key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "session = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "s3_client = boto3.client('s3', aws_access_key_id=key_id, aws_secret_access_key=secret_key,endpoint_url=endpoint_url,verify=False)\n",
    "buckets=s3_client.list_buckets()\n",
    "for bucket in buckets['Buckets']: print(bucket['Name'])\n",
    "s3_client.upload_file(model_file_name, bucket['Name'],model_file_name)\n",
    "[item.get(\"Key\") for item in s3_client.list_objects_v2(Bucket=bucket['Name']).get(\"Contents\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a691644e-3956-4472-bd51-a92c4631893d",
   "metadata": {},
   "source": [
    "# Convert the model from onnx to OpenVINO IR and copy to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7f5b3-285e-47c2-a825-5429b495576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openvino-dev\n",
    "!mo --input_model cifar10.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af937768-f71d-49fd-bb49-068f2d427361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from boto3 import session\n",
    "key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "session = boto3.session.Session(aws_access_key_id=key_id, aws_secret_access_key=secret_key)\n",
    "s3_client = boto3.client('s3', aws_access_key_id=key_id, aws_secret_access_key=secret_key,endpoint_url=endpoint_url,verify=False)\n",
    "buckets=s3_client.list_buckets()\n",
    "s3_client.upload_file(model_name+\".bin\", bucket['Name'],model_name+\"/\"+model_name+\".bin\")\n",
    "s3_client.upload_file(model_name+\".xml\", bucket['Name'],model_name+\"/\"+model_name+\".xml\")\n",
    "[item.get(\"Key\") for item in s3_client.list_objects_v2(Bucket=bucket['Name']).get(\"Contents\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fd588-8f58-417c-906e-c5a1e8e08fdb",
   "metadata": {},
   "source": [
    "Deploy the model in your Data Science project using RHODS UI under \"Models and model servers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697e7f6-86d6-4810-a0be-25f3bd9ad3cb",
   "metadata": {},
   "source": [
    "# Submit HTTP REST request to the ModelMesh for single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56387f1d-26b0-4b0c-b774-7a68c3123db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"cifar10\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "URL='http://modelmesh-serving.huggingface.svc.cluster.local:8008/v2/models/'+model_name+'/infer' # underscore characters are removed\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "for imagenum in range(10):\n",
    "    img, label = testX[imagenum]\n",
    "    arr=transform(img).reshape(1,3,32,32)\n",
    "    payload = {\n",
    "        \"inputs\": [{ \"name\": \"input_0\", \"shape\": (1,3,32,32), \"datatype\": \"FP32\", \"data\": arr.tolist()}]\n",
    "    }\n",
    "    res = requests.post(URL, json=payload, headers=headers)\n",
    "    #print(res)\n",
    "    #print(res.text)\n",
    "    print(\"Expected\",label_names[label],\", Actual\",label_names[np.argmax(res.json()['outputs'][0]['data'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a497a8-f137-4f0b-9227-0e72a1f8a0fc",
   "metadata": {},
   "source": [
    "# Submit HTTP REST request to the ModelMesh for a batch of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988c792-d0f4-451d-8254-bbd7cd0846b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[np.array(transform(testX[i][0])) for i in range(0,10)]\n",
    "actual=[testX[i][1] for i in range(0,10)]\n",
    "arr=np.array(arr)\n",
    "payload = {\n",
    "        \"inputs\": [{ \"name\": \"input_0\", \"shape\": (10,3,32,32), \"datatype\": \"FP32\", \"data\": arr.tolist()}]\n",
    "}\n",
    "res = requests.post(URL, json=payload, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfb8ec0-43b5-46a7-b6b3-d07908454528",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected=np.argmax(np.array(res.json()['outputs'][0]['data']).reshape(res.json()['outputs'][0]['shape']),axis=1).tolist()\n",
    "print(actual)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb2921-842d-4f2e-97cc-5f627f60525a",
   "metadata": {},
   "source": [
    "# Submit gRPC request to the ModelMesh for single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54272a0b-94e7-4f06-94ef-a68d1004512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install grpcio grpcio-tools==1.46.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f738e-2a19-4f26-add8-fed32d1a0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/kserve/modelmesh-serving/main/fvt/proto/kfs_inference_v2.proto\n",
    "!python3 -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. ./kfs_inference_v2.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef29b9-b6a9-4faf-9e2f-9bd2df0df326",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"cifar10\"\n",
    "img, label = testX[0]\n",
    "print(img,label)\n",
    "arr=transform(img).reshape(1,3,32,32)\n",
    "payload = { \"model_name\": model_name,\n",
    "            \"inputs\": [{ \"name\": \"input_0\", \"shape\": (1,3,32,32), \"datatype\": \"FP32\", \"contents\": {\"fp32_contents\":arr.flatten().tolist()}}]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a82ed-2401-47ae-8def-4a38eb7eb998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "import kfs_inference_v2_pb2, kfs_inference_v2_pb2_grpc\n",
    "grpc_url=\"modelmesh-serving.huggingface.svc.cluster.local:8033\"\n",
    "request=kfs_inference_v2_pb2.ModelInferRequest(model_name=model_name,inputs=payload[\"inputs\"])\n",
    "grpc_channel = grpc.insecure_channel(grpc_url)\n",
    "grpc_stub = kfs_inference_v2_pb2_grpc.GRPCInferenceServiceStub(grpc_channel)\n",
    "response = grpc_stub.ModelInfer(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950d62e-4869-4ef3-95e7-419000708ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(response.outputs),type(response.raw_output_contents))\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "d = MessageToDict(response.outputs[0])\n",
    "print(d)\n",
    "binary_data=bytes([x for x in response.raw_output_contents[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee0358-ff2f-4419-8add-081f0e2f25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import base64\n",
    "FLOAT = 'f'\n",
    "fmt = '<' + FLOAT * (len(binary_data) // struct.calcsize(FLOAT))\n",
    "print(\"Expected\",label,\"Actual\",np.argmax(np.array(struct.unpack(fmt, binary_data))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ae72ec8d55aeb4773d9bab14ab14ec6c410f2dd8be83850b7c2732f479ead773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
